Implementation of Medprompt from ["Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine" (Nori et al 2023)](https://arxiv.org/abs/2311.16452).

## Status
I still need to implement the following from the paper:
- [ ] Use models own generated rationales as exemplars, instead of the ones provided by the dataset. I may buy these off of OpenRouter or whoever has free credits.
- [ ] Majority voting over 5 attempts, with answer choices shuffled each time (latter is only applicable to multiple-choice questions?)
- [ ] Use MultiMedQA as the dataset.
- [ ] Fix base model output formatting (all instruct models are fine, and Qwen2 base is fine).

In addition, I want to continue tweaking:
- [ ] For local testing on MacBook, vendor https://github.com/willccbb/mlx_parallm for batch generation.
- [ ] Add a reranker to the retrieval step.
- [ ] Fine-tune the embedding model and/or re-ranking model for task performance
- [ ] Test more embedding models and generative models, especially newer ones that are not in mlx_lm yet.
- [ ] Test on more datasets.
- [ ] Merge with STaR, i.e. use Medprompt to generate the synthetic dataset for expert iteration.
- [ ] Try MCTS for the synthetic rationale search (Medprompt/STaR are more like MCS).

## Usage
```sh
pip install -r requirements.txt
python embed.py /path/to/MATH/train embeddings.json
cd ../..
python -m llllvvuu.medprompt.mlx-eval \
  /path/to/mlx/model \
  llllvvuu/medprompt/embeddings.json \
  /path/to/MATH/train /path/to/MATH/test 1
```

## (WIP) Results

All models are quantized using `mlx_lm.convert -q`. No tools are enabled in any of the trials.

Unfortunately the confidence intervals on these are quite wide, I would need to use cloud compute to run the full test split.

### MATH

| Generative Model                 | Embedding Model | Reranking Model | Zero-Shot | Zero-Shot (Majority Vote of 5) | 5-Shot (No Reranking) | 5-Shot (No Reranking, Majority Vote of 5) | 5-Shot (w/ Reranking, Majority Vote of 5) |
|----------------------------------|-----------------|-----------------|-----------|--------------------------------|-----------------------|-------------------------------------------|-------------------------------------------|
| Gemma 2 2B Instruct @ 8bits      | bge-small       | N/A             | 3/32      |                                |                       |                                           |                                           |
| Llama 3.1 8B Instruct @ 8bits    | bge-small       | N/A             | 13/32     |                                | 11/32[^1]             |                                           |                                           |
| Llama 3.1 8B Base @ 8bits        | bge-small       | N/A             |           |                                | 5/32[^1]              |                                           |                                           |
| Qwen2 Math 1.5B Instruct @ 4bits | bge-small       | N/A             |           |                                |                       |                                           |                                           |

[^1]: Suboptimal exemplars were used in this experiment. The optimal exemplars would be generated by the model itself, but I just used the ones provided by the dataset.
